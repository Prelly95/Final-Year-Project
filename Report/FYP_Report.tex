\documentclass{UoNMCHA}
\usepackage[authoryear]{natbib}
\usepackage{array,booktabs} % For nice tables
\usepackage{amsmath,amsfonts,amssymb} % For nice maths
\usepackage{color}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage[parfill]{parskip}   % For replacing paragraph indenting with a newline instead
% Number equations per section
\numberwithin{equation}{section}

\hypersetup{
%    bookmarks=true,         % show bookmarks bar?
%    unicode=false,          % non-Latin characters in AcrobatÕs bookmarks
%    pdftoolbar=true,        % show AcrobatÕs toolbar?
%    pdfmenubar=true,        % show AcrobatÕs menu?
%    pdffitwindow=false,     % window fit to page when opened
%    pdfstartview={FitH},    % fits the width of the page to the window
%    pdftitle={My title},    % title
%    pdfauthor={Author},     % author
%    pdfsubject={Subject},   % subject of the document
%    pdfcreator={Creator},   % creator of the document
%    pdfproducer={Producer}, % producer of the document
%    pdfkeywords={keyword1} {key2} {key3}, % list of keywords
%    pdfnewwindow=true,      % links in new window
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=blue,          % color of internal links
    citecolor=blue,        % color of links to bibliography
%    filecolor=magenta,      % color of file links
    urlcolor=blue           % color of external links
}

\definecolor{MATLABKeyword}{rgb}{0,0,1}
\definecolor{MATLABComment}{rgb}{0.1328125,0.54296875,0.1328125}
\definecolor{MATLABString}{rgb}{0.625,0.125,0.9375}

\lstset{language=Matlab,
    basicstyle=\small\ttfamily,
    keywordstyle=\color{MATLABKeyword},
    %identifierstyle=,
    commentstyle=\color{MATLABComment},
    stringstyle=\color{MATLABString},
    numberstyle=\tiny,
    %numbers=left,
    basewidth=0.5em}

\firstpage{1}    % Set page number for first page
\UoNMCHAreportNo{MECH4841 Part B} %Report number
\UoNMCHAyear{2013}   % Year
\shorttitle{FYP Report - Optical Flow Based Obstacle Avoidance} %For odd pages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\includepdf[pages=-]{FYP_Cover_Page.pdf}
\title{Optical Flow Based Obstacle Avoidance for a Fixed Wing UAV\\ \ \\
{\small Final Year Project Report - MECH4841 Part B  \\June 2019}}
\author[UoNMCHA]{Patrick Prell}
\address[UoNMCHA]{
Student of Mechatronics Engineering,\\
The University of Newcastle, Callaghan, NSW 2308, AUSTRALIA \\
Student Number: 3204734 \\
E-mail: \href{mailto:Patrick.Prell@uon.edu.au}{\textsf{Patrick.Prell@uon.edu.au}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle
\onecolumn

\vspace{-5mm}
\section*{Abstract}
\vspace{-3mm}
Remember that executive summary may include the following information:
\begin{itemize}
    \item Defines the intention of the report.
    \item Places the report in context so the reader knows why it is important to read it.
    \item Why is it important?
    \item What problem is addressed?
    \item Briefly states the results
    \item Briefly presents the implications and recommendations
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-2mm}
\section*{Acknowledgements}
\vspace{-3mm}
You may like to say thank you to someone that helped you with your project.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
To organise your introduction section you can use the following structure:
\begin{itemize}
    \item \textbf{Position}: Show there is a problem and that it is important to solve it.
    \item \textbf{Problem}: Describe the specifics of the problem you are trying to address
    \item \textbf{Proposal}: Discuss how you are going to address this problem. Use the literature to back-up your approach to the problem, or to highlight that what you are doing has not been done before
\end{itemize}

The rest of the report is organised as follows. Section~\ref{sec:Background} describes items related to the core content. Section~\ref{sec:Conclusion} concludes the report. Appendix~ shows an example of how to make a Table.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}\label{sec:Background}
%TODO: Pretty good, get someone to read
To the average human or most mammals for that matter, visual tasks such as identifying objects and interpreting visual cues seem like a trivial task. However, implementing algorithms that mimic a mammalian visual cortex is infuriatingly difficult and largely remains an unsolved problem \citep{hartley2003multiple}. One fact often forgotten is that for all of its amazing abilities approximately 1/3 of the human brain is dedicated to analysing the information collected from our eyes. This statistic helps to put into perspective how challenging the field of computer vision can be.

%TODO: From part A Fix this up
The concept of computer vision has been explored since the inception of artificial intelligence in the late 1960’s. Researchers the goal of extracting the 3-Dimensional structure of a scene from images. The following decade saw research attempting to extract more information from a sequence of images using visual cues from the motion of a camera. This brought about work on visual navigation as a viable method to navigate an environment. Using Visual Navigation on a robot to move through an unstructured environment with no prior knowledge of the map is often based on optical flow (Altshuler and Srinivasan, 2018), where a motion field is estimated from fusing the motion of the robot with the motion of the pixels of a video feed. Furthermore, biologically inspired visual navigation techniques have been a topic for research. By observing the way that animals and insects utilise their visual systems to move about and map their environments, the processes for which this is achieved can be replicated and used on a robotic system for similar tasks. This section outlines the planar and spherical model of optical flow, the tools used to infer motion between frames, and how honeybees use Visual Navigation (Srinivasan, 2011).
\subsection{Biological Inspiration} % 1 page
\subsubsection{Visual Navigation}
\subsubsection{}
\subsubsection{}
\subsection{Optical Flow} % 2 pages %TODO: From part A Fix this up
Optical flow has featured in many industrial projects and applications, these industries include but are not limited to robotics and computer vision, filmmaking and video compression. Filmmakers and the film industry use optical flow to track targets, augment the colour and texture of objects and add 3D effects to a preshot scene. A major part of optical flow reasearch is in motion estimation for video compression. This is a technique that uses a predivtive and interpolative encoding based on the optical flow motion field \citep{le1991MPEG}. %TODO:Something about the robotics industry
There have been many methods proposed to calculate the optical flow between sequential frames with varying levels of success. Depending on the goal 
\subsubsection{Horn \& Schunk} %TODO: From part A Fix this up
Optical flow is an ill-posed problem, meaning that there is only one independent measurement available from the image sequence and we require 2 variables to describe the flow at a point. B.K.P. Horn and B.G. Schunck developed an algorithm for computing optical flow in 1980 \citep{horn1981determining} by adding a second constraint known as the brightness constancy. The brightness constancy constraint assumes that the pixel intensities remain constant over time.

\begin{equation}
	\frac{d}{dt} I(x(t), y(t), t) = 0
\end{equation}
where $I(x(t), y(t), t)$ is the pixel intencity of the image at time $t$.

Horn and Schunck's optical flow algorithm was a huge step forward in computer vision research, although this algorithm is limited in that it typically can only estimate small motions, the method fails in the presence of large motion when the gradient of the image is not smooth enough \citep{meinhardt2013horn}.
\subsubsection{Lucas - Kanade} %TODO: From part A Fix this up
The Lucas-Kanade method is a widely used method for calculating optical flow. This method operates under the assumption that the flow is effectively constant in a small cluster of pixels, the method can then solve the optical flow equation for all the pixels in that neighbourhood using least squares regression. The Lucas-Kanade method improves on previous methods by using the spatial intensity gradient to find the disparity vector h, reducing the algorithms calculation time to $O(M^2log(N))$ for the average case.

\subsubsection{What is Observable in Optical Flow}
Pose rate estimation

\begin{itemize}
	\item Angular velocity,
	\item reflect on the work presented, 
	\item make recommendations,
	\item suggest future work or improvements.
\end{itemize}

\newpage
\subsection{Camera Calibration} %TODO: Pretty good, get someone to read
Computer vision begins with the camera detecting light from a scene. This light has travelled from its source, reflecting from an object, through the camera lens and is recorded on the image sensor. For these rays of light that make up an image to be useful for representing the real-world, we must understand the geometry that transforms these rays from the three-dimensional real world to a two-dimensional image plane i.e. world coordinates to image coordinates.

The camera calibration process describes the camera with two sets of parameters, namely the intrinsic parameters, and the extrinsic parameters. The intrinsic parameters describe the geometric relationship between the camera model and its projection. And the extrinsic parameters describe the location and orientation or pose, of the camera with respect to the world frame.

This project uses a GoPro Hero 7 to test and verify the calibration routine. This section will cover camera models and outline why a planar model is used for the GoPro. It covers the mathematical tools used to capture the different effects that make our camera imperfect for representing the real world, and how we can correct for these deviations.
\subsubsection{Camera Models}
The most elementry way of representing a camera is with the pinhole model. Light enters the appeture (pinhole) after reflecting from objects in a scene. For an ideal pinhole camera, only one ray enters the apeture from any point in the scene. The light is then projected onto a surface behind the appeture resulting in an image of the scene that is scaled proportional to the focal length $f$.

\begin{figure}[ht]
    \begin{center}
        \includegraphics[width=.8\linewidth]{Figures/Pinhole_model}
        \caption{Elementary pinhole model of a camera: In practice, the appeture must be big enough to let light in, allowing a narrow ray of light from each point in the scene blurring the image. Therefore there is a tradeoff between brightness and sharpness.}
        \label{fig:PinholeModel}
    \end{center}
\end{figure}

Figure~\ref{fig:PinholeModel} shows an ideal pinhole camera model and its parameters, where $f$ is the focal length, $Z$ is the distance along the optical axis to an object, $x$ is the size of the projected object and $X$ is the size of the object. For this model, \eqref{eq:pinholeEQ} is the relationship between the projected image and the object
\begin{equation}\label{eq:pinholeEQ}
	\frac{-x}{f} = \frac{X}{Z}
\end{equation}
Due to how light enters the camera, the image projection from a pinhole model is projected upside down. This issue is fixed in a planar camera model. The planar model is similar to a pinhole model, however rather than passing through an appeture, light travels from each point in the scene, intersects the image plane and converges to a single point known as the \textit{Centre of projection}. Figure~\ref{fig:PlainarModel} shows an ideal planar camera model. It better represents the geometry of a camera with a lens, albeit a camera with an imposibly perfect lens.

\begin{figure}[ht]
	\begin{minipage}[c]{0.3\linewidth}
		\centering
		\begin{tabular}{l l}
			$C$ &: Centre of projection\\
			$\{c\}$ &: Camera co-ordinates\\
			$P$ &: Point in world space\\
			$P'$ &: Point on image plane\\
			$\vec{u}, \vec{v}$ &: Image co-ordinates\\
			$f$ &: Focal length\\
			$I$ &: Image plane
		\end{tabular}
	\end{minipage}
	\begin{minipage}[c]{0.65\linewidth}
		\includegraphics[width=1\linewidth]{Figures/Planar_Model}	
	\end{minipage}
\caption{Ideal camera model. A point $P$ in camera co-ordinates \{$\vec{c_{1}}, \vec{c_{2}}, \vec{c_{3}}$\} is projected and displayed as a point $p$ in image co-ordinates \{$\vec{u}, \vec{v}$\}}
\label{fig:PlainarModel}
\end{figure}
The idea behind modeling and calibrating a camera is to identify and cancel out imperfections caused by slight defects in the manufacturing process. Firstly, the image sensor is often notmounted inlign with the optical center of the lense. Therefor, we introduce $\alpha_{x}$ and $\alpha_{x}$ to model a displacement from the optical centre. Furthermore, each pixel is not necessarily square causing a small discrepancy between the focal lenght in the $\vec{u}$ and $\vec{v}$ directions, captured in the $f_{u}$ and $f_{v}$ parameters. The following equations~\ref{eq:ProjectionEquations} project a point $P_{c_{1}, c_{2}, c_{3}}$ to image sensor co-ordinates $u, v$:
\begin{equation*}\label{eq:ProjectionEquations}
	\begin{split}
		u = &f_{u} \times \frac{\vec{c_{1}}}{\vec{c_{3}}} + \alpha_{u}\\
		v = &f_{v} \times \frac{\vec{c_{2}}}{\vec{c_{3}}} + \alpha_{v}
	\end{split}
\end{equation*}
\subsubsection{Homogenius Coordinates}

\subsubsection{Lens Distortion}

\subsection{Inverse Optimal Control} % 3 pages
\subsubsection{Sudo Control Algorithm for Insects}
\subsubsection{}
\subsection{Divergence}
 
\subsection{Approach}
\subsubsection{Inverse Depth Calculation}
\subsubsection{Unity Simulation}
\subsubsection{Assumptions}
As discussed in 

Static environment
using GoPro
Simple aircraft kinematic model
\subsubsection{Simulations}
Kinematic data collected
Used ray cast to gather depth information
\subsubsection{Real World Data}
The project is primaritly simmulation based. However, the camera calibration routine (explored in following sections was tested and evaluated on real world images collected from a GoPro Hero 7. Were this project to be implemented on a real world fixed wing UAV, the kinematic data would need to be obtained through sensors on the UAV. Additionally, the control algorithm
GoPro Hero
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Optical Flow - Simulation}
\subsection{Measurement Model}
\subsection{Farnback Algorithm - OpenCV}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Camera Calibration}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Vector Field Divergence for Object Avoidance}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Flow Magnatude for Object Avoidance}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Conclusion}\label{sec:Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Recomendations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\bibliographystyle{harvard}
\bibliography{main} % This is the .bib file where the bibliography database is stored

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\newpage
\section{Journal}\label{app:Journal}

\end{document}